{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.1"
  },
  "name": "",
  "signature": "sha256:7e454ee55a83b49e731bb1b690109321a5d42d17b5d9546735ed6f18dccc1d2e"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "K-Means Clustering"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The clustering method k-means aims to partition N observations into K clusters in which each observation belongs to the cluster with the nearest mean. The problem is NP hard. \n",
      "\n",
      "####Mathematical background\n",
      "\n",
      "The k-means algorithm takes a dataset X of N points as input, together with a parameter K specifying how many clusters to create. The output is a set of K cluster centroids and a labeling of X that assigns each of the points in X to a unique cluster. All points within a cluster are closer in distance to their centroid than they are to any other centroid.\n",
      "\n",
      "The mathematical condition for the K clusters $C_k$ and the K centroids $\\mu_k$ can be expressed as: \n",
      "\n",
      "Minimize $\\displaystyle \\sum_{k=1}^K \\sum_{\\mathrm{x}_n \\in C_k} ||\\mathrm{x}_n - \\mu_k ||^2$ with respect to $\\displaystyle C_k, \\mu_k$\n",
      "\n",
      "####Lloyd's algorithm\n",
      "\n",
      "An iterative method known as the k-means algorithm or Lloyd\u2019s algorithm exists that converges (albeit to a local minimum) in few steps. The procedure alternates between two operations. \n",
      "\n",
      "1. Once a set of centroids $\\mu_k$ is available, the clusters are updated to contain the points closest in distance to each centroid. \n",
      "2. Given a set of clusters, the centroids are recalculated as the means of all points belonging to a cluster.\n",
      "\n",
      "$$\\displaystyle C_k = \\{\\mathrm{x}_n : ||\\mathrm{x}_n - \\mu_k|| \\leq \\mathrm{\\,\\,all\\,\\,} ||\\mathrm{x}_n - \\mu_l||\\}\\qquad(1)$$\n",
      "\n",
      "$$\\displaystyle \\mu_k = \\frac{1}{C_k}\\sum_{\\mathrm{x}_n \\in C_k}\\mathrm{x}_n\\qquad(2)$$\n",
      "\n",
      "The two-step procedure continues until the assignments of clusters and centroids no longer change. As already mentioned, the convergence is guaranteed but the solution might be a local minimum. In practice, the algorithm is run multiple times and averaged. For the starting set of centroids, several methods can be employed, for instance random assignation.\n",
      "\n",
      "The k-means algorithm works reasonably well when the data fits the cluster model:\n",
      "\n",
      "* The clusters are spherical: the data points in a cluster are centered around that cluster\n",
      "* The spread/variance of the clusters is similar: Each data point belongs to the closest cluster\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "%matplotlib inline\n",
      "import os, sys, random\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from matplotlib import animation\n",
      "from shapely.geometry import Point, MultiPoint"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "# Add path to JSAnimation to pythonpath \n",
      "# git clone https://github.com/jakevdp/JSAnimation.git\n",
      "# Save JSAnimation in geodata/src\n",
      "JSAnimation_path = os.path.join('./', '../src/JSAnimation')\n",
      "sys.path.append(JSAnimation_path)\n",
      "from JSAnimation import IPython_display"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "A class implementation of the k-means algorithm"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "# Class that implements the K-Means clustering algorithm\n",
      "class KMeans():\n",
      "    def __init__(self, K, N=None, init_data='random', X=None, verbose=True):\n",
      "        \n",
      "        # Init points -- in the domain (-1, 1)x(-1, 1)\n",
      "        if init_data == 'random':\n",
      "            try:\n",
      "                self.X = [Point(random.uniform(-1, 1), random.uniform(-1, 1)) for i in range(N)]\n",
      "            except TypeError:\n",
      "                print('N needs to be provided if init_data=random')\n",
      "                return\n",
      "        elif init_data == 'file':\n",
      "            # Override any given N, if any\n",
      "            N = None\n",
      "            self.X = []\n",
      "            for line in open('X2.dat'):\n",
      "                x, y = map(float, line.strip().split(','))\n",
      "                self.X.append(Point(x, y))\n",
      "        elif init_data == 'data':\n",
      "            if not X:\n",
      "                print('X needs to be specified when initializing with data')\n",
      "                return\n",
      "            # Override any given N, if any\n",
      "            N = None\n",
      "            self.X = X\n",
      "        else:\n",
      "            print('Method invalid')\n",
      "            return\n",
      "        \n",
      "        # Set attributes\n",
      "        self.K = K\n",
      "        self.N = N or len(self.X)\n",
      "        self.MAX_ITERATIONS = 100\n",
      "        self.verbose = verbose\n",
      "\n",
      "        # Sanity check for input parameters\n",
      "        if self.N <= 1:\n",
      "            print('Try N > 1 for a meaningful problem.')\n",
      "            return\n",
      "        if self.K > self.N:\n",
      "            print('The number of target clusters is larger than the number of available points. Try K <= N.')\n",
      "            return\n",
      "        \n",
      "        print('Initialized problem with {} points to be clustered to {}'.format(self.N, self.K)) if self.verbose else ''\n",
      "        \n",
      "    # Function to initialize centroids randomly among the data points\n",
      "    def _init_centroids(self):\n",
      "        print('Initialized centroids assigning to random dataset points.') if self.verbose else ''\n",
      "        return {label: centroid for label, centroid in enumerate(random.sample(self.X, self.K))}\n",
      "        \n",
      "    # Function to assign points to each centroid\n",
      "    def _cluster_points(self):\n",
      "        # We create a dictionary where key is the cluster's label\n",
      "        clusters = {}\n",
      "        for point in self.X:\n",
      "            closest_centroid_label = min([(label, point.distance(centroid)) \\\n",
      "                                           for label, centroid in self.centroids.items()], \n",
      "                                           key=lambda x:x[1])[0]\n",
      "            # Directly append each point to the list corresponding to its label\n",
      "            # If cluster not populated yet, it'll except with KeyError and will create it\n",
      "            # (EAFP way: easier to ask for forgiveness than permission)\n",
      "            try:\n",
      "                clusters[closest_centroid_label].append(point)\n",
      "            except KeyError:\n",
      "                clusters[closest_centroid_label] = [point]\n",
      "        self.clusters = clusters\n",
      "        \n",
      "    def _reassign_centroids(self):\n",
      "        # Create a MultiPoint with all points in each cluster. New centroid can be trivially computed\n",
      "        centroids = {}\n",
      "        # Method .items() of dictionary returns an iterator on (key, values)\n",
      "        for label, cluster in self.clusters.items():\n",
      "            centroids[label] = self.mp(cluster).centroid\n",
      "        self.centroids = centroids\n",
      "        \n",
      "    def _has_converged(self):\n",
      "        # Decides whether the algorithm has reached an end point\n",
      "        # Conditions to exit: \n",
      "        # (1) reach max number of allowed iterations\n",
      "        # (2) algorithm has converged and found a stable solution\n",
      "        if self.iteration > self.MAX_ITERATIONS: \n",
      "            return False\n",
      "        # In the first iteration, old_centroids has not been set yet\n",
      "        # Except with AttributeError (EAFP way: easier to ask for forgiveness than permission)\n",
      "        try:\n",
      "            return set(tuple(c.coords) for c in self.centroids.values()) == \\\n",
      "                    set(tuple(c.coords) for c in self.old_centroids.values())\n",
      "        except AttributeError:\n",
      "            return False\n",
      "\n",
      "    def initialize(self, plot=False):\n",
      "        \n",
      "        # Initialize attributes to empty state (allows to rerun the .run() method on the same initial data)\n",
      "        self.iteration = 0\n",
      "        self.centroids = None\n",
      "        self.clusters = None\n",
      "        self.data = []\n",
      "        \n",
      "        # Store initial data (for later viz). Note that there're no centroids in this 1st step\n",
      "        this_data = {0: {'cluster': self.X}}\n",
      "        self.data.append(this_data)\n",
      "        \n",
      "        # Initialize centroids\n",
      "        self.centroids = self._init_centroids()\n",
      "        \n",
      "        # To visualize initialization of centroids\n",
      "        if plot:\n",
      "            fig = plt.figure(figsize=(4, 4))\n",
      "            ax = fig.add_axes([0, 0, 1, 1], xlim=(-1, 1), ylim=(-1, 1), aspect='equal', xticks=[], yticks=[])\n",
      "            \n",
      "            # Plot data points\n",
      "            ax.plot([p.x for p in self.X], [p.y for p in self.X], 'ro', markersize=4, alpha=0.3)\n",
      "            \n",
      "            # Plot centroids, annotate them according to the order in which they were drawn\n",
      "            # Sorted list by kmeans.centroid keys\n",
      "            centroids = [c[1] for c in sorted(list(self.centroids.items()))]\n",
      "            ax.plot([c.x for c in centroids], [c.y for c in centroids], 'bo', markersize=6, alpha=1)\n",
      "            for i, c in enumerate(centroids, 1):\n",
      "                ax.text(c.x, c.y, str(i), fontsize=20)\n",
      "                \n",
      "    def run(self):\n",
      "        \n",
      "        # Loop until algorithm converges\n",
      "        while not self._has_converged():\n",
      "            # Update counter\n",
      "            self.iteration += 1\n",
      "\n",
      "            # Copy existing centroids to old_centroids\n",
      "            self.old_centroids = self.centroids\n",
      "            \n",
      "            # Assign all points in X to clusters\n",
      "            self._cluster_points()\n",
      "            \n",
      "            # Store data for plotting. Each cluster has an integer label and two components: centroid and cluster\n",
      "            # centroid is a point and cluster a list of points\n",
      "            this_data = {}\n",
      "            for label in self.centroids:\n",
      "                this_data[label] = {'centroid': self.centroids[label], 'cluster': self.clusters[label]}\n",
      "            self.data.append(this_data)\n",
      "            \n",
      "            # Reevaluate centroids according to the new clusters\n",
      "            self._reassign_centroids()\n",
      "            \n",
      "        # Exit\n",
      "        print('Clustered {} points to {} clusters in {} iterations. Bye'.\n",
      "              format(self.N, self.K, self.iteration-1)) if self.verbose else ''\n",
      "    \n",
      "    def animator(self):\n",
      "        \n",
      "        # Utility function to create an animated video of the evolution\n",
      "        # Initialize figure and axes. \n",
      "        fig = plt.figure(figsize=(6, 6))\n",
      "        ax = fig.add_axes([0, 0, 1, 1], xlim=(-1, 1), ylim=(-1, 1), aspect='equal', xticks=[], yticks=[])\n",
      "        ax.spines['bottom'].set_linewidth(3)\n",
      "        ax.spines['right'].set_linewidth(3)\n",
      "        \n",
      "        # Create a list of K colors taken from a particular colormap\n",
      "        # See http://wiki.scipy.org/Cookbook/Matplotlib/Show_colormaps\n",
      "        colors = plt.cm.spectral([1.*m/(self.K-1) for m in range(self.K)])\n",
      "        \n",
      "        # stars is the list of K layers for the centroids, pnts is for the clusters\n",
      "        # Initialize with empty 2d data and the desired marker, size and transparency\n",
      "        stars = sum([ax.plot([], [], '*', markersize=10, alpha=1) for k in range(self.K)], [])\n",
      "        pnts = sum([ax.plot([], [], '.', markersize=8, alpha=0.6) for k in range(self.K)], [])\n",
      "        \n",
      "        def _init():\n",
      "            # The initial data are all the unclustered points\n",
      "            data_snapshot = self.data[0]\n",
      "            # Set color to red and add the only cluster (all points) to the layer\n",
      "            pnts[0].set_color('red')\n",
      "            cluster = data_snapshot[0]['cluster']\n",
      "            pnts[0].set_data([c.x for c in cluster], [c.y for c in cluster])\n",
      "            return pnts\n",
      "\n",
      "        def _animate(i):\n",
      "            # On first iteration don't do anything, that's the initial data\n",
      "            if i == 0:\n",
      "                return\n",
      "            # For each subsequent iteration, get the snapshot of the data\n",
      "            data_snapshot = self.data[i]\n",
      "            \n",
      "            # Populate stars with centroid data and pnts with cluster points\n",
      "            for k, (star, pnt) in enumerate(zip(stars, pnts)):\n",
      "                # Choose always the same color for the same label!\n",
      "                c = colors[k]\n",
      "                centroid = data_snapshot[k]['centroid']\n",
      "                cluster = data_snapshot[k]['cluster']\n",
      "                # Set color to the markers\n",
      "                star.set_color(c)\n",
      "                pnt.set_color(c)\n",
      "                # Centroid is a single point\n",
      "                star.set_data(centroid.x, centroid.y)\n",
      "                # Cluster is a list of points. .plot() takes arrays of x and y positions\n",
      "                pnt.set_data([c.x for c in cluster], [c.y for c in cluster])\n",
      "            return stars + pnts\n",
      "        \n",
      "        # Create an animation of as many frames as iterations, interval in ms\n",
      "        # blit=True only re-draws parts of plot that change\n",
      "        anim = animation.FuncAnimation(fig, _animate, init_func=_init,\n",
      "                                       frames=self.iteration, interval=600, blit=False)\n",
      "        return anim\n",
      "        \n",
      "    # Turns a list of geometric point features into a MultiPoint (for easy viz)\n",
      "    def mp(self, list_of_points):\n",
      "        return MultiPoint([p for p in list_of_points])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Running k-means on random data with random initialization of centroids\n",
      "\n",
      "In a random data sample there is no obvious cluster structure. The solution is alike to solving for a continuous geometric region, giving raise to a Voronoi tessellation of space. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "kmeans = KMeans(K=5, N=500)\n",
      "kmeans.initialize(plot=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "kmeans.run()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "anim = kmeans.animator()\n",
      "IPython_display.display_animation(anim, default_mode='once')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Due to the random initialization of centroids, each instantiation of the algorithm leads to a different solution. Potentially, only local minima are reached. \n",
      "\n",
      "Compare a second run over the same data and spot the differences in final configuration:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "kmeans.initialize(plot=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "kmeans.run()\n",
      "animb = kmeans.animator()\n",
      "IPython_display.display_animation(animb, default_mode='once')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Observe the Voronoi tessellation by running with a large number of points (careful about performance!)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "kmeans2 = KMeans(K=7, N=3000)\n",
      "kmeans2.initialize()\n",
      "kmeans2.run()\n",
      "anim2 = kmeans2.animator()\n",
      "IPython_display.display_animation(anim2, default_mode='once')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Running k-means on globular data with random initialization of centroids\n",
      "\n",
      "In a data sample with defined cluster-like structures, the k-means algorithm initialized to random centroids may produce a suboptimal solution if the initial random draw is unfortunate, which is an obvious drawback of the initialization method."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "kmeans3 = KMeans(K=3, init_data='file')\n",
      "kmeans3.initialize(plot=True)\n",
      "kmeans3.run()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "anim3 = kmeans3.animator()\n",
      "IPython_display.display_animation(anim3, default_mode='once')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Compare with a different evolution on the same data."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "kmeans3.initialize(plot=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "kmeans3.run()\n",
      "anim3b = kmeans3.animator()\n",
      "IPython_display.display_animation(anim3b, default_mode='once')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###k-means++: the advantages of careful seeding\n",
      "\n",
      "A solution to the inadequacy of the random initialization of centroid, called k-means++, was proposed in 2007 by [Arthur and Vassilvitskii](http://theory.stanford.edu/~sergei/papers/kMeansPP-soda.pdf). This algorithm comes with a theoretical guarantee to find a solution that is $O(log k)$ competitive to the optimal k-means solution. It is also fairly simple to describe and implement. Starting with a dataset X of N points $(\\mathrm{x}_1, \\ldots, \\mathrm{x}_N)$,\n",
      "\n",
      "* choose an initial center $c_1$ uniformly at random from X. Compute the vector containing the square distances between all points in the dataset and $c_1: D_i^2 = ||\\mathrm{x}_i - c_1 ||^2$\n",
      "* choose a second center $c_2$ from X randomly drawn from the probability distribution $D_i^2 / \\sum_j D_j^2$\n",
      "* recompute the distance vector as $D_i^2 = \\mathrm{min} \\left(||\\mathrm{x}_i - c_1 ||^2, ||\\mathrm{x}_i - c_2 ||^2\\right)$\n",
      "* choose a successive center $c_l$ and recompute the distance vector as $D_i^2 = \\mathrm{min} \\left(||\\mathrm{x}_i - c_1 ||^2, \\ldots, ||\\mathrm{x}_i - c_l ||^2\\right)$\n",
      "* when exactly k centers have been chosen, finalize the initialization phase and proceed with the standard k-means algorithm\n",
      "\n",
      "To implement the a probability distribution of step 2, we compute the cumulative probabilities for choosing each of the N points in X. These cumulative probabilities are partitions in the interval [0,1] with length equal to the probability of the corresponding point being chosen as a center, as explained in this stackoverflow thread. Therefore, by picking a random value $r \\in [0,1]$ and finding the point corresponding to the segment of the partition where that $r$ value falls, we are effectively choosing a point drawn according to the desired probability distribution."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class KPlusPlus(KMeans):\n",
      "    \n",
      "    def _dist_from_centroids(self, centroids):\n",
      "    \n",
      "        # Compute distance vector: \n",
      "        # for each point in X, find the minimum of the square distances to each of the available centroids\n",
      "        D2 = np.array([min([x.distance(c)**2 for c in centroids.values()]) for x in self.X])\n",
      "        self.D2 = D2\n",
      " \n",
      "    def _choose_next_centroid(self):\n",
      "        \n",
      "        # Construct the probability distribution D2_i/sum(D2_j)\n",
      "        probs = self.D2/self.D2.sum()\n",
      "        # Compute the cumulative distribution\n",
      "        cumprobs = probs.cumsum()\n",
      "        # Find in which partition of the [0, 1] interval according to the cum distribution a random number falls\n",
      "        r = random.random()\n",
      "        ind = np.where(cumprobs >= r)[0][0]\n",
      "        return(self.X[ind])\n",
      " \n",
      "    def _init_centroids(self):\n",
      "        \n",
      "        # Draw first centroid randomly\n",
      "        centroids = {0: random.sample(self.X, 1)[0]}\n",
      "        drawn_centroids = len(centroids.values())\n",
      "        # Continue drawing until having K centroids\n",
      "        while drawn_centroids <= self.K:\n",
      "            self._dist_from_centroids(centroids)\n",
      "            centroids[drawn_centroids-1] = self._choose_next_centroid()\n",
      "            drawn_centroids += 1\n",
      "            \n",
      "        print('Initialized centroids assignment using k++') if self.verbose else ''\n",
      "\n",
      "        return centroids"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "kpp = KPlusPlus(K=3, init_data='file')\n",
      "kpp.initialize(plot=True)\n",
      "kpp.run()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "animpp = kpp.animator()\n",
      "IPython_display.display_animation(animpp, default_mode='once')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "kpp.initialize(plot=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Finding the K in k-means clustering\n",
      "\n",
      "Clustering consist of grouping objects in sets, such that objects within a cluster are as similar as possible, whereas objects from different clusters are as dissimilar as possible. Thus, the optimal clustering is somehow subjective and dependent on the characteristic used for determining similarities, as well as on the level of detail required from the partitions. \n",
      "\n",
      "A particular disadvantage of the kmeans algorithm is the need for an a priori specification of the target number of clusters K. Is it possible to determine this number from the data per se?\n",
      "\n",
      "An example: is the data from file composed of two or three clusters?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "kpp2 = KPlusPlus(K=2, init_data='file')\n",
      "kpp2.initialize()\n",
      "kpp2.run()\n",
      "animpp2 = kpp2.animator()\n",
      "IPython_display.display_animation(animpp2, default_mode='once')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A promising approach to determine K is presented in a publication by [Pham, Dimov and Nguyen](http://www.ee.columbia.edu/~dpwe/papers/PhamDN05-kmeans.pdf). The article is very much worth reading, as it includes an explanation of the drawbacks of the standard k-means algorithm as well as a comprehensive survey on different methods that have been proposed for selecting an optimal number of clusters.\n",
      "\n",
      "The goal of the authors is to define a function $f(K)$ to evaluate the quality of the clustering for different values of K. \n",
      "\n",
      "The distorsion of a cluster is a measure of the distance between points in a cluster and its centroid:\n",
      "\n",
      "$$\\displaystyle I_j = \\sum_{\\mathrm{x}_i \\in C_j} ||\\mathrm{x}_i - \\mu_j ||^2.$$\n",
      "\n",
      "The global impact of all clusters\u2019 distortions is given by the quantity\n",
      "\n",
      "$$\\displaystyle S_k = \\sum_{j=1}^K I_j.$$\n",
      "\n",
      "The authors finally arrive at the following definition of $f(K)$.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from IPython.display import Image\n",
      "Image(filename='fk.png', width=400) "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$N_d$ is the number of dimensions (attributes) of the data set (2 in our case of 2-d points) and $\\alpha_K$ is a weight factor. With this definition, $f(K)$ is the ratio of the real distortion to the estimated distortion and decreases when there are areas of concentration in the data distribution. Values of K that yield small $f(K)$ can be regarded as giving well-de\ufb01ned clusters."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class KFinder():\n",
      "    def __init__(self, init_data='data', kmax=6, N=None, X=None, verbose=False):\n",
      "        rangeK = range(1, kmax+1)\n",
      "        kruns = []\n",
      "        if init_data == 'data':\n",
      "            # To do, exercise\n",
      "            pass\n",
      "        elif init_data in ['random', 'file']:\n",
      "            for thisk in rangeK:\n",
      "                kruns.append(KPlusPlus(K=thisk, N=N, init_data=init_data, verbose=verbose))\n",
      "        \n",
      "        self.rangeK = rangeK\n",
      "        self.kruns = kruns\n",
      "        self.verbose = verbose\n",
      "        \n",
      "    def _fK(self, krun, thisk, Skm1=0):\n",
      "        # Compute f(K)\n",
      "        # Nd is hardcoded to 2 in our case with 2d points\n",
      "        Nd = 2\n",
      "        # Recursive definition of alpha\n",
      "        a = lambda k, Nd: 1 - 3/(4*Nd) if k == 2 else a(k-1, Nd) + (1-a(k-1, Nd))/6\n",
      "\n",
      "        centroids = krun.centroids.values()\n",
      "        clusters = krun.clusters.values()\n",
      "\n",
      "        # Compute Sk quantity as sum of all clusters' distorsions\n",
      "        Sk = sum([sum([p.distance(krun.centroids[i]) for p in krun.clusters[i]]) for i in range(thisk)])\n",
      "\n",
      "        if thisk == 1:\n",
      "            fs = 1\n",
      "        elif Skm1 == 0:\n",
      "            fs = 1\n",
      "        else:\n",
      "            fs = Sk/(a(thisk,Nd)*Skm1)\n",
      "        return fs, Sk \n",
      "    \n",
      "    def run(self):\n",
      "        # We will store results as tuple of tuples [k, f(K)]\n",
      "        fks = []\n",
      "        \n",
      "        # Run k++ for each of the target Ks\n",
      "        for thisk, krun in zip(self.rangeK, self.kruns):\n",
      "            krun.initialize(plot=self.verbose)\n",
      "            krun.run()\n",
      "            \n",
      "            # Handle special k=1 case\n",
      "            print('Computing fK for k={} using krun with k={}'.format(thisk, krun.K)) if self.verbose else ''\n",
      "            if thisk == 1:\n",
      "                fk, Sk = self._fK(krun, thisk)\n",
      "            else:\n",
      "                # Note that the value stored in variable Sk when thisk = K corresponds to SK-1\n",
      "                # Therefore we are passing SK-1 as argument for the f(K) calculation and updating Sk in the same step\n",
      "                fk, Sk = self._fK(krun, thisk, Sk)\n",
      "            \n",
      "            # Store results\n",
      "            fks.append([thisk, fk])\n",
      "            \n",
      "        self.fks = fks"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "kfinder = KFinder(init_data='file', kmax=8, verbose=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "kfinder.run()"
     ],
     "language": "python",
     "metadata": {
      "scrolled": true
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "kfinder.fks"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The f(K) paper suggests that values of f(K) < 0.85 are evidence of a potentially good value of K. \n",
      "\n",
      "In the case of our data we find a best value K=2, followed by a second best choice K=3."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "fig = plt.figure(figsize=(6, 4))\n",
      "ax = fig.add_axes([0, 0, 1, 1])\n",
      "ax.plot([k[0] for k in kfinder.fks], [k[1] for k in kfinder.fks], 'ro-', markersize=4, alpha=0.8)\n",
      "ax.set_xlabel('Number of clusters K', fontsize=14)\n",
      "ax.set_ylabel('Function f(K)', fontsize=14)\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Exercise\n",
      "\n",
      "Repeat this analysis using initial data with diverse cluster structures and find out which optimal K is found by the f(K) method."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}
